[
  {
    "objectID": "careers.html",
    "href": "careers.html",
    "title": "Careers",
    "section": "",
    "text": "Meridian Labs is a 501(c)(3) non-profit building open source tools for AI research and evaluation based in Boston, Massachusetts. All positions at Meridian Labs include:\nMeridian Labs has built or contributed to many important open source packages used by Governments, Labs, and Researchers to better measure and understand AI systems. Join us to help contribute to a platform which is supporting the responsible development of AI technology."
  },
  {
    "objectID": "careers.html#open-positions",
    "href": "careers.html#open-positions",
    "title": "Careers",
    "section": "Open Positions",
    "text": "Open Positions\n\nSenior Front-End Engineer\nBuild the interfaces that help AI researchers make sense of model behavior. You’ll ship production features in TypeScript and React, own significant portions of our UI, and engage directly with our open source community. 5+ years experience required.\n\n\nPrincipal Front-End Engineer\nDesign and build the interfaces that help AI researchers make sense of model behavior. You’ll ship production features in TypeScript and React, own significant portions of our UI, and engage directly with our open source community. 10+ years experience required.\n\n\nPrincipal Full-Stack Engineer\nDesign and build features that help AI researchers interact with and understand model behavior throughout our stack. You’ll own major systems and shape how researchers interact with evaluation data. 10+ years experience required."
  },
  {
    "objectID": "careers/principal-fe.html",
    "href": "careers/principal-fe.html",
    "title": "Principal Front-End Engineer",
    "section": "",
    "text": "Meridian Labs is building an open source platform for the research and evaluation of large language models. We’re looking for a Principal Front-End Engineer to drive the development of the interfaces that researchers use to interact with complex evaluation data through tools like Inspect Viewer and Inspect Viz.\nYou’ll help build the products that AI researchers use to make sense of model behavior—turning dense evaluation results into clear, actionable insights. This role combines deep technical leadership with product sensibility and stewardship of our open source community.\n\nResponsibilities\n\nBuild and ship production features using TypeScript and React\nOwn major systems and make high-impact technical decisions that shape the product\nMentor other engineers and elevate the team’s front-end capabilities\nDrive product direction by synthesizing user feedback and identifying opportunities\nSteward our open source community—setting standards, reviewing PRs, and engaging contributors\nCommunicate technical strategy to leadership and stakeholders\n\n\n\nMinimum Qualifications\n\n10+ years of software engineering experience\n7+ years building production web applications with TypeScript and React\nExperience leading front-end architecture for complex applications\nTrack record of owning and delivering major product initiatives\nHistory of mentoring engineers and influencing technical direction\nStrong interpersonal skills and collaborative mindset\nBachelor’s degree in CS or related field (or equivalent experience)\n\n\n\nPreferred Qualifications\n\nExperience with Python and comfort contributing across the stack\nBackground in data visualization or complex UI for technical users\nExperience leading open source projects or communities\nFamiliarity with AI safety, model evaluation, or ML operations\nExperience with Inspect AI or similar evaluation platforms\n\n\n\nLocation\nMeridian Labs is based in Boston, MA. We prefer in-person or hybrid work.\n\n\nCompensation\n$225,000 – $325,000 with benefits, commensurate with experience.\n\n\nApplying\nTo apply, please email the following to jobs@meridianlabs.ai:\n\nYour resume\nLinkedIn URL (if available)\nGitHub URL (if available)\nA brief summary of why you’re interested in joining Meridian Labs"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Meridian Labs",
    "section": "",
    "text": "Open source tools for frontier AI research and evaluation\n\n\nMeridian Labs is a 501(c)(3) non-profit building open source tools for AI research and evaluation. AI progress is accelerating, with new developments emerging at a breakneck pace. Meridian’s tools help organizations make empirically grounded assessments of the capabilities, opportunities, and risks of frontier models."
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "Meridian Labs",
    "section": "Projects",
    "text": "Projects\n\n\n\n  \n    \n        Inspect AI\n         \n        \n            \n            \n            \n        \n            \n            \n            \n        \n    \n\n    \n    Inspect AI is an open-source framework developed by the UK AI Safety Institute (AISI) with Meridian Labs as contributors. This framework enables systematic evaluation of large language models (LLMs), providing researchers, developers, and policy makers with the tools to conduct rigorous, repeatable assessments of AI capabilities and behaviors. The platform enables standardized testing protocols to track model progress, identify potential risks, and ensure that AI systems are reliable and aligned with human values before deployment.\n\n  \n\n  \n    \n        Inspect VSCode\n         \n        \n            \n            \n            \n        \n            \n            \n            \n        \n    \n\n    \n    The Inspect AI Visual Studio Code extension makes it simple and productive to use Inspect AI directly in the VSCode IDE. Includes features like an integrated log viewer, task browser, panels for configuring .env files and CLI options, as well as command for running and debugging tasks.\n\n  \n\n  \n    \n        Inspect Viz\n         \n        \n            \n            \n            \n        \n            \n            \n            \n        \n    \n\n    \n    Inspect Viz is a a data visualisation library for Inspect AI. Inspect Viz provides flexible tools for creating high quality interactive visualisations from Inspect evaluations. Inspect Viz includes a support for a variety of built in visualizations for helping understand evaluation results as well as components to make it easy to create custom visualizations from Inspect Data.\n  \n\n  \n    \n        Inspect SWE\n         \n        \n            \n            \n            \n        \n            \n            \n            \n        \n    \n\n    \n    The inspect_swe package makes software engineering agents like Claude Code and Codex CLI available as standard Inspect Agents. This allows researchers to easily evaluate the performance of these agents on a wide variety of tasks using the Inspect AI framework.\n  \n\n\nNo matching items"
  },
  {
    "objectID": "mission.html",
    "href": "mission.html",
    "title": "Mission",
    "section": "",
    "text": "Frontier AI research and development is accelerating, with new capabilities emerging at a breakneck pace. These systems offer unprecedented opportunities for scientific breakthroughs, educational transformation, and technological advancement. However, as AI systems become more powerful, their dual-use nature becomes more apparent - the same capabilities that could advance medicine or cybersecurity could also lower barriers to developing harmful biological or chemical agents or conducting cyberattacks. This underscores the importance of understanding these systems thoroughly, as they can both create and help defend against potential threats.\nOrganizations worldwide are grappling with these opportunities and risks as they make critical decisions about AI development and governance. To navigate these challenges, organizations are developing systematic approaches to evaluate AI models, with evaluation software and tools serving as essential enablers of this work. Government agencies, including those in the US and UK, have begun conducting joint evaluations of frontier AI models prior to their release , demonstrating the growing importance of robust evaluation frameworks.\nLeading AI developers now routinely conduct extensive pre-release evaluations to measure capabilities and assess risks, as evidenced by detailed system cards from organizations like OpenAI and Anthropic . This practice has extended beyond industry, with civil society organizations and nonprofits increasingly engaging in independent evaluations - from RAND’s work on evaluation methodologies and benchmarking to pre-deployment testing by organizations like METR and Apollo Research.\nEven as the investment in model evaluations grows, it remains difficult and time consuming to produce reliable, actionable insights. Without substantial platform investment or large technical overhead, results suffer from inconsistent metrics, limited reproducibility, difficulty sharing evaluations work, and gaps between theoretical measures and practical implications.\nMeridian Labs addresses these challenges by developing rigorous, empirically grounded evaluation frameworks and tools. We combine technical expertise with practical implementation guidance to help organizations conduct meaningful evaluations that directly inform their AI development and governance decisions."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Meridian Labs",
    "section": "",
    "text": "a 501(c)(3) nonprofit. 501 Boylston St 10th Floor Boston, MA 02116"
  },
  {
    "objectID": "team.html",
    "href": "team.html",
    "title": "Team",
    "section": "",
    "text": "Meet the dedicated experts behind Meridian Labs.\n\n\n\n  \n    \n      Alexandra Abbas \n      \n      \n        \n          \n        \n      \n        \n          \n        \n      \n      \n      (Technical Program Manager)\n    \n    \n    Alexandra is Technical Program Manager at Meridian Labs, directing projects that advance AI evaluations through scalable systems. Prior to joining Meridian, Alexandra co-founded the AI Safety Engineering Taskforce in 2024. She has lead the development of Inspect Evals, an open-source repository of 70+ AI evaluations, and has collaborated with the UK AI Security Institute, Vector Institute, and METR..\n\nPreviously, Alexandra was Engineering Lead at Wise, managing a team of 5 engineers on projects that saved millions in foreign exchange losses. She has published first-author research on AI evaluations and adversarial training, and holds an MSc in Big Data Engineering from the University of Stirling.\n\n  \n\n  \n    \n      Eric Patey \n      \n      \n        \n          \n        \n      \n        \n          \n        \n      \n      \n      (Principal Engineer)\n    \n    \n    Eric co-founded Meridian Labs in 2025, bringing extensive expertise in software architecture and engineering leadership. Most recently, he served as Distinguished Engineer at Sonos after leading their entry into B2B as Senior Director for Sonos for Business.\n\nPreviously, Eric co-founded Talko Inc. as VP of Development, was Partner Software Engineering Manager at Microsoft across Xbox and Office platforms, and was a co-founder/VP of Development at Groove Networks (acquired by Microsoft), was a founding engineer at SilverStream Software and led development at Iris Associates (Lotus Notes).\n\n  \n\n  \n    \n      Ransom Richardson \n      \n      \n        \n          \n        \n      \n        \n          \n        \n      \n      \n      (Principal Engineer)\n    \n    \n    Ransom joined Meridian Labs in 2025 as Principal Researcher where he works to advance AI evaluation platform technology. Prior to joining Meridian, Ransom served as Principal Software Architect at Microsoft, where he led the technical design of Copilot Pages and directed the development of Microsoft Loop Copilot, delivering features with exceptional customer feedback.\n\nPreviously, Ransom built a machine learning team focused on improving collaboration in Microsoft Office, partnered with the Microsoft AI Development Acceleration Program to develop early-in-career AI talent at Microsoft, shipped Azure ML FPGA inferencing, co-founded Talko Inc. (acquired by Microsoft) as service team lead, and was an engineer at Groove Networks where he designed their peer-to-peer synchronization engine. He holds 20+ patents and published research in cryptography.\n\n  \n\n  \n    \n      Charles Teague \n      \n      \n        \n          \n        \n      \n        \n          \n        \n      \n      \n      (CEO)\n    \n    \n    Charles co-founded Meridian Labs in 2025 to advance the public understanding of AI systems through comprehensive evaluation tools and research. Prior to founding Meridian, Charles was a Technology and Security Policy Fellow at the RAND Corporation, where he helped develop Inspect AI, an open-source platform for evaluating large language models, helping build frameworks and methodologies to improve AI evaluation infrastructure.\n\nPreviously, Charles was Software Architect at Posit PBC, where he was a creator and maintainer of Quarto, a widely-adopted tool for scientific and technical communication. Before his work in open source software, Charles founded and led Lose It! as CEO for over a decade, growing the weight loss platform to 30 million users before its acquisition by EverydayHealth in 2022. Earlier in his career, Charles co-founded Onfolio (acquired by Microsoft in 2006), where he led engineering for Windows Live Writer, and served as Engineering Manager at Allaire/Macromedia, directing the ColdFusion MX team. He has founded and exited multiple companies to Macromedia, Microsoft, and EverydayHealth.\n\n  \n\n\nNo matching items"
  },
  {
    "objectID": "careers/senior-fe.html",
    "href": "careers/senior-fe.html",
    "title": "Senior Front-End Engineer",
    "section": "",
    "text": "Meridian Labs is building an open source platform for the research and evaluation of large language models. We’re looking for a Senior Front-End Engineer to help build the interfaces that researchers use to interact with complex evaluation data through tools like Inspect Viewer and Inspect Viz.\nYou’ll help build the products that AI researchers use to make sense of model behavior—turning dense evaluation results into clear, actionable insights. This role combines front-end expertise with product sensibility and direct engagement with our user community.\n\nResponsibilities\n\nBuild and ship production features in Inspect’s front-end applications using TypeScript and React\nOwn significant portions of our UI codebase and make sound technical decisions\nCollaborate with internal engineers and gather feedback from external users to inform product direction\nDesign compelling, usable interfaces for technically sophisticated users\nParticipate in our open source community—reviewing PRs and engaging contributors\nCommunicate progress and decisions to leadership and stakeholders\n\n\n\nMinimum Qualifications\n\n5+ years of software engineering experience\n3+ years building production web applications with TypeScript and React\nExperience owning and shipping significant front-end features\nTrack record of executing projects with multiple stakeholders\nStrong interpersonal skills and collaborative mindset\nBachelor’s degree in CS or related field (or equivalent experience)\n\n\n\nPreferred Qualifications\n\nExperience with Python and comfort contributing to backend systems\nBackground in data visualization or complex UI for technical users\nExperience with open source communities\nFamiliarity with AI safety, model evaluation, or ML operations\n\n\n\nLocation\nMeridian Labs is based in Boston, MA. We prefer in-person or hybrid work.\n\n\nCompensation\n$150,000 – $225,000 with benefits, commensurate with experience.\n\n\nApplying\nTo apply, please email the following to jobs@meridianlabs.ai:\n\nYour resume\nLinkedIn URL (if available)\nGitHub URL (if available)\nA brief summary of why you’re interested in joining Meridian Labs"
  },
  {
    "objectID": "careers/principal-fs.html",
    "href": "careers/principal-fs.html",
    "title": "Principal Full-Stack Engineer",
    "section": "",
    "text": "Meridian Labs is building an open source platform for the research and evaluation of large language models. We’re looking for a Principal Full-Stack Engineer to contribute to development across our platform — from the React-based interfaces researchers use daily to the Python infrastructure that powers evaluation workflows.\nYou’ll help build the vision and architecture for how AI researchers interact with and understand model behavior. This role combines deep technical leadership across the stack with product sensibility and stewardship of our open source community.\n\nResponsibilities\n\nBuild and ship production features using Python and related technologies\nOwn major systems and make high-impact technical decisions that shape the product\nMentor other engineers and elevate the team’s capabilities across the stack\nDrive product direction by synthesizing user feedback and identifying opportunities\nSteward our open source community—setting standards, reviewing PRs, and engaging contributors\nCommunicate technical strategy to leadership and stakeholders\n\n\n\nMinimum Qualifications\n\n10+ years of software engineering experience\n3+ years building production systems in Python\nExperience leading architecture for complex full-stack applications\nTrack record of owning and delivering major product initiatives\nHistory of mentoring engineers and influencing technical direction\nStrong interpersonal skills and collaborative mindset\nBachelor’s degree in CS or related field (or equivalent experience)\n\n\n\nPreferred Qualifications\n\nBackground in data visualization or complex UI for technical users\nExperience leading open source projects or communities\nFamiliarity with AI safety, model evaluation, or ML operations\nExperience with Inspect AI or similar evaluation platforms\nExperience with cloud infrastructure and deployment\n\n\n\nLocation\nMeridian Labs is based in Boston, MA. We prefer in-person or hybrid work.\n\n\nCompensation\n$225,000 – $325,000 with benefits, commensurate with experience.\n\n\nApplying\nTo apply, please email the following to jobs@meridianlabs.ai:\n\nYour resume\nLinkedIn URL (if available)\nGitHub URL (if available)\nA brief summary of why you’re interested in joining Meridian Labs"
  }
]