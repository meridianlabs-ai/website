[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Meridian Labs",
    "section": "",
    "text": "Open source tools for frontier AI research and testing.\n\n\nFronter AI research and development is accelerating, with new capabilities emerging at a breakneck pace. To better understand the opportunities and risks of frontier models, organizations are using evaluations to make emperically grounded assessments. Meridian Labs helps enable this work by developing open source evaluation frameworks and tools."
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "Meridian Labs",
    "section": "Projects",
    "text": "Projects\n\n\n\n  \n    \n        Inspect AI\n         \n        \n            \n            \n            \n        \n            \n            \n            \n        \n    \n\n    \n    Inspect AI is an open-source framework developed by the UK AI Safety Institute (AISI) with Meridian Labs as contributors. This framework enables systematic evaluation of large language models (LLMs), providing researchers, developers, and policy makers with the tools to conduct rigorous, repeatable assessments of AI capabilities and behaviors. The platform enables standardized testing protocols to track model progress, identify potential risks, and ensure that AI systems are reliable and aligned with human values before deployment.\n\n  \n\n  \n    \n        Inspect VSCode\n         \n        \n            \n            \n            \n        \n            \n            \n            \n        \n    \n\n    \n    The Inspect AI Visual Studio Code extension makes it simple and productive to use Inspect AI directly in the VSCode IDE. Includes features like an integrated log viewer, task browser, panels for configuring .env files and CLI options, as well as command for running and debugging tasks.\n\n  \n\n  \n    \n        Inspect Viz\n         \n        \n            \n            \n            \n        \n            \n            \n            \n        \n    \n\n    \n    Inspect Viz is a a data visualisation library for Inspect AI. Inspect Viz provides flexible tools for creating high quality interactive visualisations from Inspect evaluations. Inspect Viz includes a support for a variety of built in visualizations for helping understand evaluation results as well as components to make it easy to create custom visualizations from Inspect Data.\n  \n\n  \n    \n        Inspect SWE\n         \n        \n            \n            \n            \n        \n            \n            \n            \n        \n    \n\n    \n    The inspect_swe package makes software engineering agents like Claude Code and Codex CLI available as standard Inspect Agents. This allows researchers to easily evaluate the performance of these agents on a wide variety of tasks using the Inspect AI framework.\n  \n\n\nNo matching items"
  },
  {
    "objectID": "mission.html",
    "href": "mission.html",
    "title": "Mission",
    "section": "",
    "text": "Fronter AI research and development is accelerating, with new capabilities emerging at a breakneck pace. These systems offer unprecedented opportunities for scientific breakthroughs, educational transformation, and technological advancement. However, as AI systems become more powerful, their dual-use nature becomes more apparent - the same capabilities that could advance medicine or cybersecurity could also lower barriers to developing harmful biological or chemical agents or conducting cyberattacks. This underscores the importance of understanding these systems thoroughly, as they can both create and help defend against potential threats.\nOrganizations worldwide are grappling with these opportunities and risks as they make critical decisions about AI development and governance. To navigate these challenges, organizations are developing systematic approaches to evaluate AI models, with evaluation software and tools serving as essential enablers of this work. Government agencies, including those in the US and UK, have begun conducting joint evaluations of frontier AI models prior to their release , demonstrating the growing importance of robust evaluation frameworks.\nLeading AI developers now routinely conduct extensive pre-release evaluations to measure capabilities and assess risks, as evidenced by detailed system cards from organizations like OpenAI and Anthropic . This practice has extended beyond industry, with civil society organizations and nonprofits increasingly engaging in independent evaluations - from RANDâ€™s work on evaluation methodologies and benchmarking to pre-deployment testing by organizations like METR and Apollo Research.\nEven as the investment in model evaluations grows, it remains difficult and time consuming to produce reliable, actionable insights. Without substantial platform investment or large technical overhead, results suffer from inconsistent metrics, limited reproducibility, difficulty sharing evaluations work, and gaps between theoretical measures and practical implications.\nMeridian Labs addresses these challenges by developing rigorous, empirically grounded evaluation frameworks and tools. We combine technical expertise with practical implementation guidance to help organizations conduct meaningful evaluations that directly inform their AI development and governance decisions."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Meridian Labs",
    "section": "",
    "text": "501 Boylston St 10th Floor Boston, MA 02116\nMeridian Labs is a 501(c)(3) nonprofit."
  },
  {
    "objectID": "team.html",
    "href": "team.html",
    "title": "Team",
    "section": "",
    "text": "Meet the dedicated experts behind Meridian Labs.\n\n\n\n  \n    \n      Charles Teague \n      \n      \n        \n          \n        \n      \n        \n          \n        \n      \n      \n      (CEO)\n    \n    \n    Charles co-founded Meridian Labs in 2025 to advance the public understanding of AI systems. Prior to founding Meridian, Charles was a TASP Fellow at the RAND Corporation, where he focused on frameworks and methdologies for evaluating large language models, helping build the Inspect AI platform.\n\nPrior to his work in open source software, Charles built products used by millions of people and founded companies with successful exits to Microsoft, Macromedia, and EverydayHealth.\n\n  \n\n  \n    \n      Eric Patey \n      \n      \n        \n          \n        \n      \n        \n          \n        \n      \n      \n      (Principal Researcher)\n    \n    \n    Eric co-founded Meridian Labs in 2025, bringing extensive expertise in software architecture and engineering leadership. Most recently, he served as Distinguished Engineer at Sonos after leading their entry into B2B as Senior Director for Sonos for Business.\n\nPreviously, Eric co-founded Talko Inc. as VP of Development, was Partner Software Engineering Manager at Microsoft across Xbox and Office platforms, and was a co-founder/VP of Development at Groove Networks (acquired by Microsoft), was a founding engineer at SilverStream Software and led development at Iris Associates (Lotus Notes)\n\n  \n\n  \n    \n      Alexandra Abbas \n      \n      \n        \n          \n        \n      \n        \n          \n        \n      \n      \n      (Technical Program Manager)\n    \n    \n    Alexandra is Technical Program Manager at Meridian Labs, directing projects that advance AI evaluations through scalable systems. Prior to joining Meridian, Alexandra co-founded the AI Safety Engineering Taskforce in 2024. She is lead the development of Inspect Evals, an open-source repository of 70+ AI evaluations, and has collaborated with the UK AI Security Institute, Vector Institute, and METR.\n\nPreviously, Alexandra was Engineering Lead at Wise, managing a team of 5 engineers on projects that saved millions in foreign exchange losses. She has published first-author research on AI evaluations and adversarial training, and holds an MSc in Big Data Engineering from the University of Stirling.\n\n  \n\n\nNo matching items"
  }
]